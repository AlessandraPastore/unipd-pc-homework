\section{Performance}\label{performance}

Our parallel solution does not bring desired results because:
\begin{enumerate}
    \item \label{overhead-squared} One feels the added slowness of the squared version too much compared to the traditional version because of \cite{rucci}: 
    \begin{itemize}
        \item more efficient in executing long for cycles than a few short cycles
        \item more efficient compiler optimisations in long cycles than in short cycles
        \item fewer accounts to be made concerning the cells to be accessed (modules, divisions, ...)
    \end{itemize}
    \item \label{overhead-mpi} Although the work to be divided into process can be fairly parallelized between the various process in the case of large arrays, almost to the point of homogeneous use there remains the problem that \cite{koallen}:
    \begin{itemize}
        \item the problem of the main process and the worker process remain, so any communications such as scatter, gather cannot be used and only broadcast and point-to-point can be used
        \item blocking waits remain to ensure proper synchronisation between process
        \item with MPI you cannot do maximum paralellisation as with CUDA for example
    \end{itemize}
    \item \label{overhead-dependencies} Having very important dependencies, it cannot really be paralysed like other algorithms \cite{rucci}:
    \begin{itemize}
        \item dependencies must be run before running other threads and can be executed synchronously, in which case it would be good to parallelize the internal loops as with CUDA
        \item dependencies are a big block to any intelligent form of parallelizm in MPI, such as scatter, gather, and in any matrix-like algorithm allow for very good performance
    \end{itemize}
    \item \label{memory-massive} Being a memory-intensive algorithm, each copy wastes time and space \cite{maras}:
    \begin{itemize}
        \item given the inherent structure of MPI that allows it to run on different machines with non-shared memory: it is necessary, unless using a common buffer used by several machines and very slow, to make numerous memory copies, these copies should be minimised to avoid slowing down the process too much
        \item with other common memory implementations, however, it is possible to minimise these times and work on a single instance
    \end{itemize}
\end{enumerate}

In particular, our results are very similar to what the literature states: without massive execution of internal loops synchronously (i.e. with CUDA), Floyd-Warshall times remain stable even in parallel as the algorithm requires massive parallel execution without synchronisation times.

From the graph in figure \cref{time} you can see that the square version is always slower than the classical one except in the case of a really small matrix, the reasons are described in \cref{overhead-squared} of previous numbered list.

From the graph in figure \cref{time} you can see that the parallel speed always remains constant, the reasons being mainly that there are too many dependencies and MPI adds so much overhead (described in \cref{memory-massive} of previous numbered list).

Despite this, the paralell algorithm achieves speedups of up to 200\% compared to the quadratic version, as shown in \cref{speedup}, although it never beats the classic version \cref{time}. However, this speedup does not increase as processors increase, so the parallel version does not scale well.

\begin{figure}[htbp]
    \pgfplotstableread[col sep=comma]{media/time.csv}\datatable

    \begin{subfigure}[t]{0.5\textwidth}
        \subfile{media/time.tikz}        
        \caption*{Lower is better}
        \caption{Time: execution time of the various algorithms as the matrix size and step size}
        \label{time}
    \end{subfigure}
    \begin{subfigure}[t]{0.5\textwidth}
        \subfile{media/speedup.tikz}
        \caption*{Higher is better, \(100 =\) same speed as sequential squared algorithm}
        \caption{Speedup: performance gain as the number of
        processors increases: \( S ( p ) = T _ { \text {base } } / T ( p ) \)}
        \label{speedup}
    \end{subfigure}

    \caption{Performance metrics of squared paralellization in MPI}
    \label{performance-metrics}
\end{figure}

In \cref{work-and-communication-time} we can see how the workloads are distributed and how much time the processes lose in communication, we note that in the case of 4 and 8 CPUs the workload in the worker threads is very similar and homogeneous, whereas in the case of 2 CPUs it is strongly unbalanced, especially on very large arrays.

Long synchronisation times are the main reason why the Floyd-Warhsall algorithm does not scale correctly in the case of parallelism. This can only be solved through the use of massive parallel computing systems, like CUDA cores.
The main cause of slowness, especially when working with more than 2 processors, is due to the copying of buffers, as discussed in \cref{overhead-dependencies,overhead-mpi} of previous numbered list, because as can be seen, the root process is unloaded and cannot wait to work.

\begin{figure}[htbp]
    %\pgfplotstableread[col sep=comma]{media/commimpact100_20.csv}\datatable
    %\subfile{media/commimpact.tikz}   
    %\begin{subfigure}[t]{0.5\textwidth}
    %    \subfile{media/commimpact.tikz}        
    %    \caption*{Lower comunication time is better}
    %    \caption{Communication impact on the performance on 600 - 200}
    %    \label{commimpact600_24}
    %\end{subfigure}
    \pgfplotstableread[col sep=comma]{media/commimpact1000_8.csv}\datatable
    %\begin{subfigure}[t]{0.5\textwidth}
        \subfile{media/commimpact.tikz}
        \caption*{Lower comunication time is better}
    %    \caption{Communication impact on the performance on 1000 - 8}
    %    \label{commimpact1000_40}
    %\end{subfigure}

    \caption{Work and communication time in Squared Floyd-Warshall: Communication impact on the performance on 1000 - 8}
    \label{work-and-communication-time}
\end{figure}

