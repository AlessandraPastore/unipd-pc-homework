\section{Implementation in MPI}\label{implementation-in-mpi}

C++ was chosen for the MPI implementation of the programme design. In particular, a three template class was realised for the representation of the matrix information. The reason why a template class was chosen is that for saving distances, it is more correct to use a data type with a specific infinity value and not the maximum, while for saving the pred type in for homogeneity with the indices.

For the exchange of data from the main thread to the secondary threads, the choice has been made to minimise the data to be passed, and only those which are strictly necessary are passed. Data are passed with \texttt{MPI\_Bcast}, \texttt{MPI\_Send} and \texttt{MPI\_Recv} in a blocking manner.

\texttt{MPI\_Barriers} were used to perform synchronisation between the various threads and where parallelization is not possible as described in the previous paragraphs.

The parallel code returns that the parallel code is \(O(n/s \cdot (1+n/(sp)+n/(sp)+n^2/(s^2p))\cdot s^3)=O(n^3/p)\) where \(p\) the number of processors, while the sequential code is \( O(n^3 ) \).
%% TODO check

Already as can be seen from this analysis, the times remain similar to those of the classical algorithm.
This is precisely why we do not expect great performance from the parallel algorithm.

\FloatBarrier

